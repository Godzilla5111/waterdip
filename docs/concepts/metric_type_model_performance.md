# Performance Metrics

Performance metrics are measurements that quantitatively calculate your modelâ€™s performance.
The metrics consider what the model predicted (prediction) against what actually happens (Actuals).

## List of Model Performance Metrics

| **Metrics**              | **Model Family** | **Description**                                                                                                                   |
|--------------------------|------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| **Accuracy**             | Classification   | The base metric used for model evaluation is often Accuracy, describing the number of correct predictions over all predictions    |
| **Precision**            | Classification   | Precision is a measure of how many of the positive predictions made are correct (true positives)                                  |
| **Recall / Sensitivity** | Classification   | Recall is a measure of how many of the positive cases the classifier correctly predicted, over all the positive cases in the data |
| **F_1**                  | Classification   | F1-Score is a measure combining both precision and recall. It is generally described as the harmonic mean of the two              |
| **Specificity**          | Classification   | Specificity is a measure of how many negative predictions made are correct (true negatives)                                       |

